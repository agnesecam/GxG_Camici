{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f4f68c-b067-453b-8e1f-e60e04a05a20",
   "metadata": {},
   "source": [
    "# Fase 3 - SVM con embeddings ItWac 32 (in-genre classification)\n",
    "Sviluppare un classificatore basato su SVM lineari che prende in input una rappresentazione del testo costruita attraverso l’uso dei word embedding (http://www.italianlp.it/resources/italian-word-embeddings/). Riportare i seguenti risultati:\n",
    "- testare diverse rappresentazioni del testo che variano rispetto al modo di combinare gli embedding delle singole parole e/o rispetto alle categorie grammaticali delle paroleprese in considerazione. Valutare i diversi sistemi con un processo di 5-fold cross validation condotto sul training set;\n",
    "- valutazione sul test set ufficiale del miglior sistema rispetto ai risultati ottenuti con il processo di 5-fold cross validation del punto sopra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c09313-af78-4ed8-b657-173d8a5ee3bd",
   "metadata": {},
   "source": [
    "Dal task GxG Evalita 2018:\n",
    "\n",
    "\"Given a (collection of) text(s) from a specific genre, the gender of the author has to be predicted. The task is cast as a binary classification task, with gender represented as F (female) or M (male). Gender prediction will be done in two ways: \n",
    "\n",
    "1. **using a model which has been trained on the same genre**\n",
    "2. using a model which has been trained on anything but that genre.\"\n",
    "\n",
    "In questo file utilizzeremo un modello allenato sullo stesso genere su cui poi verrà testato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3b8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee5f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(src_path):\n",
    "    embeddings = dict()\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            embedding = [float(comp) for comp in embedding]\n",
    "            embeddings[word] = np.asarray(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c012f321-7798-4d06-a1ca-58c914ae25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens, pos_tags = [], []\n",
    "    for line in lines:\n",
    "        if line.startswith('#') or line.strip() == '':\n",
    "            continue\n",
    "        parts = line.strip().split('\\t')\n",
    "        if '-' in parts[0] or '.' in parts[0]:\n",
    "            continue\n",
    "        tokens.append(parts[1].lower())\n",
    "        pos_tags.append(parts[3])\n",
    "    return tokens, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c5f04f-3ec3-455d-96b0-988921268bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_filename(filename):\n",
    "    return filename.split('#')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8531b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_text(tokens, model, pos_tags=None, \n",
    "                   allowed_pos=None, strategy='mean'):\n",
    "    vectors = []\n",
    "    for token, pos in zip(tokens, pos_tags):\n",
    "        if allowed_pos and pos not in allowed_pos:\n",
    "            continue\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "    if not vectors:\n",
    "        return np.zeros(next(iter(model.values())).shape[0])\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    if strategy == 'mean':\n",
    "        return np.mean(vectors, axis=0)\n",
    "    elif strategy == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    elif strategy == 'max':\n",
    "        return np.max(vectors, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c09e923",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/embeddings/itwac32.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      2\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/embeddings/itwac\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_word_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mload_word_embeddings\u001b[1;34m(src_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_word_embeddings\u001b[39m(src_path):\n\u001b[0;32m      2\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m             line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/embeddings/itwac32.txt'"
     ]
    }
   ],
   "source": [
    "dim = 32\n",
    "embeddings_path = f'../../data/embeddings/itwac{dim}.txt'\n",
    "embeddings = load_word_embeddings(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a01447",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {'strategy': 'mean', 'allowed_pos': None, 'name': 'mean_all'},\n",
    "    {'strategy': 'mean', 'allowed_pos': {'NOUN', 'ADJ'}, 'name': 'mean_noun_adj'},\n",
    "    {'strategy': 'max', 'allowed_pos': {'VERB'}, 'name': 'max_verb'}\n",
    "]\n",
    "\n",
    "genres = ['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres:\n",
    "    print(f\"\\n=== GENRE: {genre} ===\")\n",
    "    train_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "    test_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "\n",
    "    for config in strategies:\n",
    "        print(f\"\\n[STRATEGY: {config['name']}]\")\n",
    "\n",
    "        # Diagnostica: verifica file disponibili\n",
    "        train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "        print(f\"File trovati per il training ({len(train_files)}): {train_files[:5]}{' ...' if len(train_files) > 5 else ''}\")\n",
    "\n",
    "        X_train, y_train = [], []\n",
    "        for filename in train_files:\n",
    "            label = get_label_from_filename(filename)\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_train.append(vector)\n",
    "            y_train.append(label)\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "        # Cross-validation\n",
    "        clf = LinearSVC()\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        print(\"CV Accuracy:\", scores)\n",
    "        print(\"Mean CV Accuracy:\", scores.mean())\n",
    "\n",
    "        # Test set\n",
    "        X_test, ids = [], []\n",
    "        for filename in os.listdir(test_folder):\n",
    "            if not filename.startswith('test'):\n",
    "                continue\n",
    "            file_id = filename.split('#')[1]\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(test_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_test.append(vector)\n",
    "            ids.append(file_id)\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        # Train final model and predict\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        # Save CSV in sottocartella output/predizioni\n",
    "        output = pd.DataFrame({'id': ids, 'gender': predictions})\n",
    "        output_dir = \"output_embeddings\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f'test_predictions_{genre}_{config[\"name\"]}.csv')\n",
    "\n",
    "        output.to_csv(output_file, index=False)\n",
    "        print(f\"Predizioni salvate in: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c656248-5682-480f-8e32-0b6c0d432623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzo tabella per i risultati\n",
    "report_rows = []\n",
    "\n",
    "for genre in genres:\n",
    "    print(f\"\\n=== GENRE: {genre} ===\")\n",
    "    train_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "    test_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "\n",
    "    for config in strategies:\n",
    "        print(f\"\\n[STRATEGY: {config['name']}]\")\n",
    "\n",
    "        # Caricamento dati di training\n",
    "        train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "        X_train, y_train = [], []\n",
    "        for filename in train_files:\n",
    "            label = get_label_from_filename(filename)\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_train.append(vector)\n",
    "            y_train.append(label)\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        clf = LinearSVC()\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "        print(\"CV Accuracy:\", scores)\n",
    "        print(\"Mean CV Accuracy:\", scores.mean())\n",
    "\n",
    "        # Aggiunta al report\n",
    "        report_rows.append({\n",
    "            \"genre\": genre,\n",
    "            \"strategy\": config[\"name\"],\n",
    "            \"fold1\": scores[0],\n",
    "            \"fold2\": scores[1],\n",
    "            \"fold3\": scores[2],\n",
    "            \"fold4\": scores[3],\n",
    "            \"fold5\": scores[4],\n",
    "            \"mean_accuracy\": scores.mean()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba174e-427c-455f-bac3-14b210ad5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio CSV del report\n",
    "output_dir = \"output_embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "report_df = pd.DataFrame(report_rows)\n",
    "report_path = os.path.join(output_dir, \"report_valutazione.csv\")\n",
    "report_df.to_csv(report_path, index=False)\n",
    "print(f\"\\nReport salvato in {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06797a7-ba78-4940-8d7e-cd5c5d37e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORT FINALE SUL TEST SET (usando file gold)\n",
    "test_results = []\n",
    "\n",
    "# Mappa abbreviazioni genere -> nome completo\n",
    "genre_map = {\n",
    "    \"CH\": \"children\",\n",
    "    \"DI\": \"diary\",\n",
    "    \"JO\": \"journalism\",\n",
    "    \"TW\": \"twitter\"\n",
    "}\n",
    "\n",
    "# Mappa inversa: nome completo -> abbreviazione\n",
    "inverse_map = {v: k for k, v in genre_map.items()}\n",
    "\n",
    "for filename in os.listdir(\"output_embeddings\"):\n",
    "    if not filename.startswith(\"test_predictions_\"):\n",
    "        continue\n",
    "\n",
    "    # Estrai genere e strategia dal nome file\n",
    "    parts = filename.replace(\"test_predictions_\", \"\").replace(\".csv\", \"\").split(\"_\")\n",
    "    genre = parts[0]\n",
    "    strategy = \"_\".join(parts[1:])\n",
    "    genre_abbr = inverse_map[genre]\n",
    "\n",
    "    # Predizioni\n",
    "    pred_df = pd.read_csv(os.path.join(\"output_embeddings\", filename))\n",
    "\n",
    "    # Etichette gold\n",
    "    gold_path = f\"../../data/original/gold/test_{genre_abbr}.gold\"\n",
    "    gold_df = pd.read_csv(gold_path, sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    # Allinea predizioni e gold\n",
    "    pred_ids = pred_df[\"id\"].astype(str)\n",
    "    pred_labels = pred_df[\"gender\"]\n",
    "    gold_labels = [gold_dict.get(i, \"UNK\") for i in pred_ids]\n",
    "\n",
    "    # Filtra quelli con etichetta nota\n",
    "    filtered = [(p, g) for p, g in zip(pred_labels, gold_labels) if g in {\"M\", \"F\"}]\n",
    "    if not filtered:\n",
    "        acc = 0.0\n",
    "    else:\n",
    "        y_pred, y_true = zip(*filtered)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    test_results.append({\n",
    "        \"genre\": genre,\n",
    "        \"strategy\": strategy,\n",
    "        \"test_accuracy\": acc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89aa339-8690-4de9-9e9d-1c22297e4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio CSV del report\n",
    "df_test = pd.DataFrame(test_results)\n",
    "df_test.to_csv(\"output_embeddings/risultati_test_set.csv\", index=False)\n",
    "print(\"Report test salvato in: output_embeddings/risultati_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CONFUSION MATRIX PER IL TEST SET (solo matplotlib) ===\n",
    "\n",
    "genre_map = {\n",
    "    \"CH\": \"children\",\n",
    "    \"DI\": \"diary\",\n",
    "    \"JO\": \"journalism\",\n",
    "    \"TW\": \"twitter\"\n",
    "}\n",
    "\n",
    "inverse_map = {v: k for k, v in genre_map.items()}\n",
    "\n",
    "for filename in os.listdir(\"output_embeddings\"):\n",
    "    if not filename.startswith(\"test_predictions_\"):\n",
    "        continue\n",
    "\n",
    "    parts = filename.replace(\"test_predictions_\", \"\").replace(\".csv\", \"\").split(\"_\")\n",
    "    genre = parts[0]\n",
    "    strategy = \"_\".join(parts[1:])\n",
    "    genre_abbr = inverse_map[genre]\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(\"output_embeddings\", filename))\n",
    "\n",
    "    gold_path = f\"../../data/original/gold/test_{genre_abbr}.gold\"\n",
    "    gold_df = pd.read_csv(gold_path, sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    pred_ids = pred_df[\"id\"].astype(str)\n",
    "    pred_labels = pred_df[\"gender\"]\n",
    "    gold_labels = [gold_dict.get(i, \"UNK\") for i in pred_ids]\n",
    "\n",
    "    filtered = [(p, g) for p, g in zip(pred_labels, gold_labels) if g in {\"M\", \"F\"}]\n",
    "    if not filtered:\n",
    "        print(f\"[{genre} - {strategy}] Nessuna etichetta valida trovata.\")\n",
    "        continue\n",
    "\n",
    "    y_pred, y_true = zip(*filtered)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"M\", \"F\"])\n",
    "\n",
    "    print(f\"\\nConfusion Matrix - Genere: {genre} | Strategia: {strategy}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"M\", \"F\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"{genre} - {strategy}\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169565e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# === VALUTAZIONE BASELINE CON DUMMY CLASSIFIER ===\n",
    "dummy_results = []\n",
    "\n",
    "for genre in genres:\n",
    "    print(f\"\\n=== DUMMY CLASSIFIER - GENRE: {genre} ===\")\n",
    "    train_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "    test_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "\n",
    "    train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "    X_train, y_train = [], []\n",
    "    for filename in train_files:\n",
    "        label = get_label_from_filename(filename)\n",
    "        tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "        vector = represent_text(tokens, embeddings, pos_tags, strategy='mean')  # usa 'mean' come default\n",
    "        X_train.append(vector)\n",
    "        y_train.append(label)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy.fit(X_train, y_train)\n",
    "\n",
    "    # Test set\n",
    "    X_test, ids = [], []\n",
    "    for filename in os.listdir(test_folder):\n",
    "        if not filename.startswith(\"test\"):\n",
    "            continue\n",
    "        file_id = filename.split('#')[1]\n",
    "        tokens, pos_tags = load_conllu_file(os.path.join(test_folder, filename))\n",
    "        vector = represent_text(tokens, embeddings, pos_tags, strategy='mean')\n",
    "        X_test.append(vector)\n",
    "        ids.append(file_id)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    predictions = dummy.predict(X_test)\n",
    "\n",
    "    # Etichette gold\n",
    "    genre_abbr = {\"children\": \"CH\"}[genre]\n",
    "    gold_df = pd.read_csv(f\"../data/original/gold/test_{genre_abbr}.gold\", sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    gold_labels = [gold_dict.get(str(i), \"UNK\") for i in ids]\n",
    "    filtered = [(p, g) for p, g in zip(predictions, gold_labels) if g in {\"M\", \"F\"}]\n",
    "\n",
    "    if filtered:\n",
    "        y_pred, y_true = zip(*filtered)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "    else:\n",
    "        acc = 0.0\n",
    "\n",
    "    dummy_results.append({\n",
    "        \"genre\": genre,\n",
    "        \"strategy\": \"dummy_most_frequent\",\n",
    "        \"test_accuracy\": acc\n",
    "    })\n",
    "\n",
    "# Salva il report dummy\n",
    "df_dummy = pd.DataFrame(dummy_results)\n",
    "df_dummy.to_csv(\"output_embeddings/risultati_test_set_dummy.csv\", index=False)\n",
    "print(\"Dummy classifier: report salvato in output_embeddings/risultati_test_set_dummy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
