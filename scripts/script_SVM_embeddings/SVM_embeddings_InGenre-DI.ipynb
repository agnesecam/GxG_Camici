{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f4f68c-b067-453b-8e1f-e60e04a05a20",
   "metadata": {},
   "source": [
    "# Fase 3 - SVM con embeddings ItWac 32 (in-genre classification)\n",
    "Sviluppare un classificatore basato su SVM lineari che prende in input una rappresentazione del testo costruita attraverso l’uso dei word embedding (http://www.italianlp.it/resources/italian-word-embeddings/). Riportare i seguenti risultati:\n",
    "- testare diverse rappresentazioni del testo che variano rispetto al modo di combinare gli embedding delle singole parole e/o rispetto alle categorie grammaticali delle paroleprese in considerazione. Valutare i diversi sistemi con un processo di 5-fold cross validation condotto sul training set;\n",
    "- valutazione sul test set ufficiale del miglior sistema rispetto ai risultati ottenuti con il processo di 5-fold cross validation del punto sopra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c09313-af78-4ed8-b657-173d8a5ee3bd",
   "metadata": {},
   "source": [
    "Dal task GxG Evalita 2018:\n",
    "\n",
    "\"Given a (collection of) text(s) from a specific genre, the gender of the author has to be predicted. The task is cast as a binary classification task, with gender represented as F (female) or M (male). Gender prediction will be done in two ways: \n",
    "\n",
    "1. **using a model which has been trained on the same genre**\n",
    "2. using a model which has been trained on anything but that genre.\"\n",
    "\n",
    "In questo file utilizzeremo un modello allenato sullo stesso genere su cui poi verrà testato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3b8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee5f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(src_path):\n",
    "    embeddings = dict()\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            embedding = [float(comp) for comp in embedding]\n",
    "            embeddings[word] = np.asarray(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ea264",
   "metadata": {},
   "source": [
    "## Definizione delle strategie di embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c012f321-7798-4d06-a1ca-58c914ae25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens, pos_tags = [], []\n",
    "    for line in lines:\n",
    "        if line.startswith('#') or line.strip() == '':\n",
    "            continue\n",
    "        parts = line.strip().split('\\t')\n",
    "        if '-' in parts[0] or '.' in parts[0]:\n",
    "            continue\n",
    "        tokens.append(parts[1].lower())\n",
    "        pos_tags.append(parts[3])\n",
    "    return tokens, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c5f04f-3ec3-455d-96b0-988921268bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_filename(filename):\n",
    "    return filename.split('#')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8531b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_text(tokens, model, pos_tags=None, \n",
    "                   allowed_pos=None, strategy='mean'):\n",
    "    vectors = []\n",
    "    for token, pos in zip(tokens, pos_tags):\n",
    "        if allowed_pos and pos not in allowed_pos:\n",
    "            continue\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "    if not vectors:\n",
    "        return np.zeros(next(iter(model.values())).shape[0])\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    if strategy == 'mean':\n",
    "        return np.mean(vectors, axis=0)\n",
    "    elif strategy == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    elif strategy == 'max':\n",
    "        return np.max(vectors, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "embeddings_path = f'../../data/embeddings/itwac{dim}.txt'\n",
    "embeddings = load_word_embeddings(embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d496e",
   "metadata": {},
   "source": [
    "## Estrazione delle feature e 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a01447",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {'name': 'mean_all', 'strategy': 'mean', 'allowed_pos': None},\n",
    "    {'name': 'mean_verb', 'strategy': 'mean', 'allowed_pos': ['VERB']},\n",
    "    {'name': 'mean_noun_adj', 'strategy': 'mean', 'allowed_pos': ['NOUN', 'ADJ']},\n",
    "    {'name': 'max_verb', 'strategy': 'max', 'allowed_pos': ['VERB']},\n",
    "    {'name': 'max_all', 'strategy': 'max', 'allowed_pos': None},\n",
    "    {'name': 'max_noun_adj', 'strategy': 'max', 'allowed_pos': ['NOUN', 'ADJ']}\n",
    "]\n",
    "\n",
    "genres = ['diary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = []\n",
    "for genre in genres:\n",
    "    print(f\"\\n=== GENRE: diary ===\")\n",
    "    train_folder = f\"../../data/profiling_output/diary/linguistic_annotation/diary/\"\n",
    "    test_folder = f\"../../data/profiling_output/diary/linguistic_annotation/diary/\"\n",
    "\n",
    "    for config in strategies:\n",
    "        print(f\"\\n[STRATEGY: {config['name']}]\")\n",
    "\n",
    "        # Diagnostica: verifica file disponibili\n",
    "        train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "        print(f\"File trovati per il training ({len(train_files)}): {train_files[:5]}{' ...' if len(train_files) > 5 else ''}\")\n",
    "\n",
    "        X_train, y_train = [], []\n",
    "        for filename in train_files:\n",
    "            label = get_label_from_filename(filename)\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_train.append(vector)\n",
    "            y_train.append(label)\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "        # Cross-validation\n",
    "        # raccogli risultati per valutazione\n",
    "        clf = LinearSVC()\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        results_cv.append({\n",
    "            'strategy': config['name'],\n",
    "            'mean_accuracy': scores.mean(),\n",
    "            'folds': scores.tolist()\n",
    "        })\n",
    "        \n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        print(\"CV Accuracy:\", scores)\n",
    "        print(\"Mean CV Accuracy:\", scores.mean())\n",
    "\n",
    "        # Test set\n",
    "        X_test, ids = [], []\n",
    "        for filename in os.listdir(test_folder):\n",
    "            if not filename.startswith('test'):\n",
    "                continue\n",
    "            file_id = filename.split('#')[1]\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(test_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_test.append(vector)\n",
    "            ids.append(file_id)\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        # Train final model and predict\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        # Save CSV in sottocartella output/predizioni\n",
    "        output = pd.DataFrame({'id': ids, 'gender': predictions})\n",
    "        output_dir = \"output_embeddings\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f'test_predictions_diary_{config[\"name\"]}.csv')\n",
    "\n",
    "        output.to_csv(output_file, index=False)\n",
    "        print(f\"Predizioni salvate in: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c656248-5682-480f-8e32-0b6c0d432623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results_cv = []\n",
    "# Inizializzo tabella per i risultati\n",
    "report_rows = []\n",
    "\n",
    "for genre in genres:\n",
    "    print(f\"\\n=== GENRE: diary ===\")\n",
    "    train_folder = f\"../../data/profiling_output/diary/linguistic_annotation/diary/\"\n",
    "    test_folder = f\"../../data/profiling_output/diary/linguistic_annotation/diary/\"\n",
    "\n",
    "    for config in tqdm(strategies, desc=\"Strategie\"):\n",
    "        print(f\"\\n[STRATEGY: {config['name']}]\")\n",
    "\n",
    "        # Caricamento dati di training\n",
    "        train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "        X_train, y_train = [], []\n",
    "        for filename in train_files:\n",
    "            label = get_label_from_filename(filename)\n",
    "            tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "            vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                                    allowed_pos=config['allowed_pos'],\n",
    "                                    strategy=config['strategy'])\n",
    "            X_train.append(vector)\n",
    "            y_train.append(label)\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        clf = LinearSVC()\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "        print(\"CV Accuracy:\", scores)\n",
    "        print(\"Mean CV Accuracy:\", scores.mean())\n",
    "\n",
    "        # Aggiunta al report\n",
    "        report_rows.append({\n",
    "            \"genre\": genre,\n",
    "            \"strategy\": config[\"name\"],\n",
    "            \"fold1\": scores[0],\n",
    "            \"fold2\": scores[1],\n",
    "            \"fold3\": scores[2],\n",
    "            \"fold4\": scores[3],\n",
    "            \"fold5\": scores[4],\n",
    "            \"mean_accuracy\": scores.mean()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba174e-427c-455f-bac3-14b210ad5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio CSV del report\n",
    "output_dir = \"output_embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "report_df = pd.DataFrame(report_rows)\n",
    "report_path = os.path.join(output_dir, \"report_valutazione_DI.csv\")\n",
    "report_df.to_csv(report_path, index=False)\n",
    "print(f\"\\nReport salvato in {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06797a7-ba78-4940-8d7e-cd5c5d37e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORT FINALE SUL TEST SET (usando file gold)\n",
    "test_results = []\n",
    "\n",
    "# Fissiamo direttamente il genere e la sua abbreviazione\n",
    "genre = \"diary\"\n",
    "genre_abbr = \"DI\"\n",
    "\n",
    "for filename in os.listdir(\"output_embeddings\"):\n",
    "    # Esegui solo sui file di predizione di diary\n",
    "    if not filename.startswith(\"test_predictions_diary_\"):\n",
    "        continue\n",
    "\n",
    "    # Estrai la strategia dal nome file\n",
    "    # es. \"test_predictions_diary_mean_all.csv\" -> \"mean_all\"\n",
    "    strategy = filename.replace(\"test_predictions_diary_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    # Carica le predizioni\n",
    "    pred_df = pd.read_csv(os.path.join(\"output_embeddings\", filename))\n",
    "\n",
    "    # Carica le etichette gold\n",
    "    gold_path = f\"../../data/dataset_originale/gold/test_{genre_abbr}.gold\"\n",
    "    gold_df = pd.read_csv(gold_path, sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    # Allinea predizioni e gold\n",
    "    pred_ids    = pred_df[\"id\"].astype(str)\n",
    "    pred_labels = pred_df[\"gender\"]\n",
    "    gold_labels = [gold_dict.get(i, \"UNK\") for i in pred_ids]\n",
    "\n",
    "    # Filtra solo le coppie valide\n",
    "    filtered = [(p, g) for p, g in zip(pred_labels, gold_labels) if g in {\"M\", \"F\"}]\n",
    "    if not filtered:\n",
    "        acc = 0.0\n",
    "    else:\n",
    "        y_pred, y_true = zip(*filtered)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    test_results.append({\n",
    "        \"genre\": genre,\n",
    "        \"strategy\": strategy,\n",
    "        \"test_accuracy\": acc\n",
    "    })\n",
    "\n",
    "# Se vuoi vedere subito il risultato:\n",
    "df_test = pd.DataFrame(test_results)\n",
    "display(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89aa339-8690-4de9-9e9d-1c22297e4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio CSV del report\n",
    "df_test = pd.DataFrame(test_results)\n",
    "df_test.to_csv(\"output_embeddings/risultati_test_set_DI.csv\", index=False)\n",
    "print(\"Report test salvato in: output_embeddings/risultati_test_set_DI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CONFUSION MATRIX PER IL TEST SET (solo matplotlib) ===\n",
    "\n",
    "genre_map = {\n",
    "    \"CH\": \"children\",\n",
    "    \"DI\": \"diary\",\n",
    "    \"JO\": \"journalism\",\n",
    "    \"TW\": \"twitter\"\n",
    "}\n",
    "\n",
    "inverse_map = {v: k for k, v in genre_map.items()}\n",
    "\n",
    "for filename in os.listdir(\"output_embeddings\"):\n",
    "    if not filename.startswith(\"test_predictions_\"):\n",
    "        continue\n",
    "\n",
    "    parts = filename.replace(\"test_predictions_\", \"\").replace(\".csv\", \"\").split(\"_\")\n",
    "    genre = parts[0]\n",
    "    strategy = \"_\".join(parts[1:])\n",
    "    genre_abbr = inverse_map[genre]\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(\"output_embeddings\", filename))\n",
    "\n",
    "    gold_path = f\"../../data/dataset_originale/gold/test_{genre_abbr}.gold\"\n",
    "    gold_df = pd.read_csv(gold_path, sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    print(\"Distribuzione etichette gold:\", pd.Series(gold_labels).value_counts())\n",
    "\n",
    "    \n",
    "    pred_ids = pred_df[\"id\"].astype(str)\n",
    "    pred_labels = pred_df[\"gender\"]\n",
    "    gold_labels = [gold_dict.get(i, \"UNK\") for i in pred_ids]\n",
    "\n",
    "    filtered = [(p, g) for p, g in zip(pred_labels, gold_labels) if g in {\"M\", \"F\"}]\n",
    "    if not filtered:\n",
    "        print(f\"[diary - {strategy}] Nessuna etichetta valida trovata.\")\n",
    "        continue\n",
    "\n",
    "    y_pred, y_true = zip(*filtered)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"M\", \"F\"])\n",
    "\n",
    "    print(f\"\\nConfusion Matrix - Genere: diary | Strategia: {strategy}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"M\", \"F\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"diary - {strategy}\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169565e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# === VALUTAZIONE BASELINE CON DUMMY CLASSIFIER ===\n",
    "dummy_results = []\n",
    "\n",
    "for genre in genres:\n",
    "    print(f\"\\n=== DUMMY CLASSIFIER - GENRE: {genre} ===\")\n",
    "    train_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "    test_folder = f\"../../data/profiling_output/{genre}/linguistic_annotation/{genre}/\"\n",
    "\n",
    "    train_files = [f for f in os.listdir(train_folder) if f.startswith(\"training\")]\n",
    "    X_train, y_train = [], []\n",
    "    for filename in train_files:\n",
    "        label = get_label_from_filename(filename)\n",
    "        tokens, pos_tags = load_conllu_file(os.path.join(train_folder, filename))\n",
    "        vector = represent_text(tokens, embeddings, pos_tags, strategy='mean')  # usa 'mean' come default\n",
    "        X_train.append(vector)\n",
    "        y_train.append(label)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy.fit(X_train, y_train)\n",
    "\n",
    "    # Test set\n",
    "    X_test, ids = [], []\n",
    "    for filename in os.listdir(test_folder):\n",
    "        if not filename.startswith(\"test\"):\n",
    "            continue\n",
    "        file_id = filename.split('#')[1]\n",
    "        tokens, pos_tags = load_conllu_file(os.path.join(test_folder, filename))\n",
    "        vector = represent_text(tokens, embeddings, pos_tags, strategy='mean')\n",
    "        X_test.append(vector)\n",
    "        ids.append(file_id)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    predictions = dummy.predict(X_test)\n",
    "\n",
    "    # Etichette gold\n",
    "    genre_abbr = {\"diary\": \"CH\", \"diary\": \"DI\", \"journalism\": \"JO\", \"twitter\": \"TW\"}[genre]\n",
    "    gold_df = pd.read_csv(f\"../../data/dataset_originale/gold/test_{genre_abbr}.gold\", sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "    gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "    gold_labels = [gold_dict.get(str(i), \"UNK\") for i in ids]\n",
    "    filtered = [(p, g) for p, g in zip(predictions, gold_labels) if g in {\"M\", \"F\"}]\n",
    "\n",
    "    if filtered:\n",
    "        y_pred, y_true = zip(*filtered)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "    else:\n",
    "        acc = 0.0\n",
    "\n",
    "    dummy_results.append({\n",
    "        \"genre\": genre,\n",
    "        \"strategy\": \"dummy_most_frequent\",\n",
    "        \"test_accuracy\": acc\n",
    "    })\n",
    "\n",
    "# Salva il report dummy\n",
    "df_dummy = pd.DataFrame(dummy_results)\n",
    "df_dummy.to_csv(\"output_embeddings/risultati_test_set_DI_dummy.csv\", index=False)\n",
    "print(\"Dummy classifier: report salvato in output_embeddings/risultati_test_set_DI_dummy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d6141",
   "metadata": {},
   "source": [
    "## Selezione della strategia migliore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86944e-1483-4c1b-b115-da8cdb2b9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Selezione della strategia migliore ===\n",
    "import pandas as pd\n",
    "\n",
    "df_cv = pd.DataFrame(report_rows)\n",
    "best_row = df_cv.loc[df_cv['mean_accuracy'].idxmax()]\n",
    "best_strategy = best_row['strategy']\n",
    "print(\"Strategia migliore:\", best_strategy)\n",
    "\n",
    "best_config = [cfg for cfg in strategies if cfg['name'] == best_strategy][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a73353-28cf-43d3-94f4-cda60327c824",
   "metadata": {},
   "source": [
    "## Valutazione sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322d03d-5bab-4503-98eb-a11e40492c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Valutazione sul test set solo per la strategia migliore ===\n",
    "print(f\"\\n== Valutazione sul test set per la strategia migliore: {best_config['name']} ==\")\n",
    "\n",
    "test_folder = \"../../data/profiling_output/diary/linguistic_annotation/diary/\"\n",
    "X_test, ids = [], []\n",
    "for filename in os.listdir(test_folder):\n",
    "    if not filename.startswith('test'):\n",
    "        continue\n",
    "    file_id = filename.split('#')[1]\n",
    "    tokens, pos_tags = load_conllu_file(os.path.join(test_folder, filename))\n",
    "    vector = represent_text(tokens, embeddings, pos_tags,\n",
    "                            allowed_pos=best_config['allowed_pos'],\n",
    "                            strategy=best_config['strategy'])\n",
    "    X_test.append(vector)\n",
    "    ids.append(file_id)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Usa lo stesso modello addestrato sull'intero training set (X_train, y_train)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Distribuzione predizioni:\", pd.Series(predictions).value_counts())\n",
    "\n",
    "\n",
    "# Mostra predizioni\n",
    "output = pd.DataFrame({'id': ids, 'gender': predictions})\n",
    "print(\"Prime predizioni:\")\n",
    "display(output.head())\n",
    "print(\"Distribuzione predizioni:\", output['gender'].value_counts().to_dict())\n",
    "\n",
    "\n",
    "\n",
    "# === Confusion Matrix e Classification Report ===\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Etichette gold\n",
    "genre_abbr = {\"diary\": \"DI\"}[genre]\n",
    "gold_df = pd.read_csv(f\"../../data/dataset_originale/gold/test_{genre_abbr}.gold\", sep=\"\\t\", header=None, names=[\"id\", \"gender\"])\n",
    "gold_dict = dict(zip(gold_df[\"id\"].astype(str), gold_df[\"gender\"]))\n",
    "\n",
    "gold_labels = [gold_dict.get(str(i), \"UNK\") for i in ids]\n",
    "filtered = [(p, g) for p, g in zip(predictions, gold_labels) if g in {\"M\", \"F\"}]\n",
    "\n",
    "if filtered:\n",
    "    y_pred, y_true = zip(*filtered)\n",
    "\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\n=== Accuracy totale sul test set: {acc:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"F\", \"M\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"F\", \"M\"], yticklabels=[\"F\", \"M\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nessun esempio valido per il calcolo della confusion matrix.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
